Random seed set to 42
Created CSV history file: /home/evgeny.polyachenko/CVRP/Dynamic_GraphTransformer_RL/training_gpu/results/tiny/csv/history_gt_rl.csv

============================================================
Training GT+RL
============================================================
Creating data generator...
Creating model: GT+RL
Model parameters: 780,460 total, 780,460 trainable
Configuration saved to /home/evgeny.polyachenko/CVRP/Dynamic_GraphTransformer_RL/training_gpu/results/tiny/GT_RL_training_config.yaml
Starting GPU-optimized training...
GPU Device: NVIDIA RTX A6000
  - Compute Capability: 8.6
  - Total Memory: 47.40 GB
  - Memory Fraction: 95.00%
  - Mixed Precision: Disabled
  - Current Memory: 0.00/0.00 GB (allocated/reserved)
Model moved to GPU:NVIDIA RTX A6000
Effective batch size: 512 (batch_size=512, accumulation_steps=1)

============================================================
GPU INFORMATION
============================================================
CUDA Version: 12.8
PyTorch Version: 2.8.0+cu128
Number of GPUs: 1

GPU 0: NVIDIA RTX A6000
  - Compute Capability: 8.6
  - Total Memory: 47.40 GB
  - Multiprocessors: 84
  - CUDA Cores: ~5376
  - Memory Used: 0.00/0.00 GB (allocated/reserved)
============================================================

----------------------------------------
Training GT+RL
Model architecture: GraphTransformer
Total parameters: 780,460
[INIT] Initializing GPU Manager...
[INIT] Checking if baseline needed for RL training...
[INIT] Building eval dataset: eval_batches=5, batch_size=512
