# GPU-Optimized Training for CVRP Models

This directory contains GPU-optimized training infrastructure for CVRP (Capacitated Vehicle Routing Problem) models including GAT (Graph Attention Network), GT (Graph Transformer), and DGT (Dynamic Graph Transformer).

## üöÄ Key Features

- **Mixed Precision Training**: Automatic mixed precision (FP16/FP32) for faster training and reduced memory usage
- **Memory Management**: Smart GPU memory management with configurable memory fractions
- **Non-blocking Transfers**: Asynchronous CPU-GPU data transfers for improved throughput
- **Gradient Accumulation**: Support for larger effective batch sizes through gradient accumulation
- **Rollout Baseline**: GPU-optimized rollout baseline computation for REINFORCE
- **Auto Batch Size**: Automatic batch size estimation based on available GPU memory
- **Comprehensive Monitoring**: Real-time GPU metrics including memory usage, utilization, and temperature

## üìã Requirements

- PyTorch >= 2.0.0 with CUDA support
- NVIDIA GPU with CUDA capability >= 3.5
- CUDA >= 11.7
- Python >= 3.8

## üèóÔ∏è Directory Structure

```
training_gpu/
‚îú‚îÄ‚îÄ lib/                      # GPU training library
‚îÇ   ‚îú‚îÄ‚îÄ gpu_utils.py         # GPU management utilities
‚îÇ   ‚îú‚îÄ‚îÄ advanced_trainer_gpu.py  # GPU-optimized trainer
‚îÇ   ‚îî‚îÄ‚îÄ rollout_baseline_gpu.py  # GPU-optimized baseline
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ run_training_gpu.py  # Main training script
‚îú‚îÄ‚îÄ configs/                 # Configuration files
‚îÇ   ‚îî‚îÄ‚îÄ gpu_default.yaml    # Default GPU configuration
‚îî‚îÄ‚îÄ results/                 # Training outputs
```

## üö¶ Quick Start

### Basic Training

Train a DGT model on GPU with default settings:

```bash
python training_gpu/scripts/run_training_gpu.py --model dgt
```

### Specify GPU Device

```bash
python training_gpu/scripts/run_training_gpu.py --model gat --device cuda:1
```

### Enable Mixed Precision

```bash
python training_gpu/scripts/run_training_gpu.py --model gt --mixed_precision
```

### Auto-estimate Optimal Batch Size

```bash
python training_gpu/scripts/run_training_gpu.py --model dgt --estimate_batch_size
```

## üìù Configuration

The GPU training system uses YAML configuration files. The default configuration is in `configs/default.yaml`:

```yaml
gpu:
  enabled: true
  device: "cuda:0"           # GPU device to use
  mixed_precision: true      # Enable FP16 training
  memory_fraction: 0.95      # Fraction of GPU memory to use
  pin_memory: true           # Pin memory for faster transfers
  non_blocking: true         # Non-blocking transfers
  gradient_accumulation_steps: 1  # Gradient accumulation
  num_workers: 4             # Data loading workers
  prefetch_factor: 2         # Batches to prefetch

training:
  batch_size: 512            # Batch size (can be larger for GPU)
  learning_rate: 1e-4
  num_epochs: 100
  clip_grad_norm: 1.0        # Gradient clipping
```

## üéØ Command Line Options

```bash
python training_gpu/scripts/run_training_gpu.py [OPTIONS]

Model Selection:
  --model {gat,gt,dgt,greedy}  Model type to train

Configuration:
  --config PATH                Configuration file path
  --problem_size N             Number of customers
  --batch_size N               Batch size
  --epochs N                   Number of epochs
  --lr FLOAT                   Learning rate

GPU Settings:
  --device DEVICE              GPU device (cuda:0, cuda:1, etc.)
  --mixed_precision            Enable mixed precision training
  --no_mixed_precision         Disable mixed precision
  --estimate_batch_size        Auto-estimate optimal batch size
  --memory_fraction FLOAT      GPU memory fraction (0.0-1.0)

Output:
  --output_dir PATH            Output directory
  --checkpoint_dir PATH        Checkpoint directory
  --wandb                      Enable W&B logging
  --benchmark                  Run in benchmark mode
```

## üî¨ Advanced Usage

### Multi-GPU Training (Future)

```python
# Currently single-GPU, multi-GPU support planned
config = {
    'gpu': {
        'devices': ['cuda:0', 'cuda:1'],  # Future feature
        'strategy': 'ddp'  # Distributed Data Parallel
    }
}
```

### Custom Training Loop

```python
from training_gpu.lib import GPUManager, advanced_train_model_gpu

# Initialize GPU manager
gpu_manager = GPUManager(
    device='cuda:0',
    memory_fraction=0.9,
    enable_mixed_precision=True
)

# Train model
model, history = advanced_train_model_gpu(
    model=model,
    data_generator=generator,
    config=config,
    checkpoint_dir=Path('checkpoints/')
)
```

### Memory Profiling

```python
from training_gpu.lib.gpu_utils import profile_memory_usage

# Profile memory usage of training step
result, mem_stats = profile_memory_usage(
    train_step, model, batch
)
print(f"Peak memory: {mem_stats['peak_memory_gb']:.2f} GB")
```

## üìä Performance Comparison

Typical performance improvements over CPU training:

| Model | Problem Size | CPU Time/Epoch | GPU Time/Epoch | Speedup |
|-------|-------------|----------------|----------------|---------|
| GAT   | 20 nodes    | 45s           | 8s             | 5.6x    |
| GT    | 20 nodes    | 60s           | 10s            | 6.0x    |
| DGT   | 20 nodes    | 75s           | 12s            | 6.2x    |
| GAT   | 50 nodes    | 180s          | 20s            | 9.0x    |
| GT    | 50 nodes    | 240s          | 25s            | 9.6x    |
| DGT   | 50 nodes    | 300s          | 30s            | 10.0x   |

*Results vary based on GPU model, batch size, and specific configurations.*

## üîß Troubleshooting

### CUDA Out of Memory

If you encounter OOM errors:

1. Reduce batch size:
   ```bash
   --batch_size 256
   ```

2. Reduce memory fraction:
   ```bash
   --memory_fraction 0.8
   ```

3. Enable gradient accumulation:
   ```yaml
   gpu:
     gradient_accumulation_steps: 2
   ```

4. Use automatic batch size estimation:
   ```bash
   --estimate_batch_size
   ```

### Mixed Precision Issues

If you encounter NaN losses with mixed precision:

1. Disable mixed precision:
   ```bash
   --no_mixed_precision
   ```

2. Adjust loss scaling in config:
   ```yaml
   gpu:
     loss_scale: 128  # Lower initial scale
   ```

### Slow Data Loading

If GPU utilization is low:

1. Increase number of workers:
   ```yaml
   gpu:
     num_workers: 8
   ```

2. Increase prefetch factor:
   ```yaml
   gpu:
     prefetch_factor: 4
   ```

## üìà Monitoring

### GPU Metrics

The training script logs comprehensive GPU metrics:

- Memory usage (allocated/reserved/free)
- GPU utilization percentage
- Temperature (if available)
- Throughput (instances/second)

### Weights & Biases Integration

Enable W&B logging for detailed monitoring:

```bash
python training_gpu/scripts/run_training_gpu.py --wandb
```

## üß™ Testing

Run GPU tests:

```bash
# Test GPU utilities
python -m pytest training_gpu/tests/test_gpu_utils.py

# Test training pipeline
python -m pytest training_gpu/tests/test_training.py

# Benchmark GPU vs CPU
python training_gpu/scripts/benchmark_gpu.py
```

## üéì Tips for Optimal Performance

1. **Batch Size**: Use the largest batch size that fits in memory
2. **Mixed Precision**: Enable for 2x memory savings and faster training
3. **Pin Memory**: Always enable for faster CPU-GPU transfers
4. **Data Loading**: Use multiple workers (4-8) for data loading
5. **Gradient Accumulation**: Use to simulate larger batch sizes
6. **Memory Management**: Clear cache periodically for long training runs

## üìÑ License

This GPU training infrastructure is part of the Dynamic_GraphTransformer_RL project.

## ü§ù Contributing

Contributions are welcome! Please ensure:

1. Code follows existing patterns
2. GPU memory is properly managed
3. Mixed precision compatibility
4. Comprehensive error handling
5. Documentation is updated

## üìß Contact

For issues or questions about GPU training, please open an issue in the main repository.
