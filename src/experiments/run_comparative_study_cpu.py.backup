#!/usr/bin/env python3
"""
CPU Comparative Study Orchestrator (training + evaluation + plots)

Goals
- Intuitive UX like Ver1: one command to train missing models (on CPU),
  reuse cached results if available, and produce comparison tables/plots.
- Support models:
  * dynamic_gt_rl (train via unified pipeline)
  * static_rl (train via unified pipeline)
  * pointer_rl (train via unified pipeline; optional/legacy)
  * greedy_baseline (eval-only heuristic)
  * naive_baseline (eval-only roundtrip)
  * (stubs reserved for gat_rl and gat_rl_legacy)

Behavior
- For RL models, the script looks for a cached summary JSON under the unified
  trainer's output directory. If not found (or if forced), it triggers training via
  src/training/pipeline_train.train_pipeline, then writes the summary JSON.
- For baselines, it evaluates on the same deterministic validation set used
  by the unified trainer (derived from --seed and --instances).
- Writes a consolidated CSV and a plot to match Ver1-style UX.

Outputs
- CSV: results/comparative_study_cpu.csv
- Plot: utils/plots/comparative_study_results.png
"""

import argparse
import json
import time
import sys
from pathlib import Path
from typing import Dict, List, Tuple

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import torch
from torch_geometric.data import Data

# Project root
project_root = Path(__file__).resolve().parents[2]
if str(project_root) not in sys.path:
    sys.path.append(str(project_root))
    sys.path.append(str(project_root / 'src'))

from src.models import (
    GreedyGraphTransformerBaseline,
)
from src.training.pipeline_train import train_pipeline
from src.utils.RL.euclidean_cost import euclidean_cost


def set_seed(seed: int):
    torch.manual_seed(seed)
    np.random.seed(seed)


def make_instance(num_nodes: int, capacity: float, device: torch.device, seed: int = None) -> Data:
    # Coordinates in unit square on a discrete 1..100 grid, divided by 100
    if seed is not None:
        g = torch.Generator(device=device)
        g.manual_seed(seed)
        coords = (torch.randint(1, 101, (num_nodes, 2), generator=g, device=device, dtype=torch.int64).float() / 100.0)
        demands = (torch.randint(1, 11, (num_nodes, 1), generator=g, device=device, dtype=torch.int64).float() / 10.0)
    else:
        coords = (torch.randint(1, 101, (num_nodes, 2), device=device, dtype=torch.int64).float() / 100.0)
        demands = (torch.randint(1, 11, (num_nodes, 1), device=device, dtype=torch.int64).float() / 10.0)
    # Customer demands sampled from {0.1, 0.2, ..., 1.0}; depot demand = 0
    demands[0] = 0.0

    # Fully connected directed edge attributes as Euclidean distances
    edge_idx = []
    edge_attr = []
    for i in range(num_nodes):
        for j in range(num_nodes):
            if i == j:
                continue
            edge_idx.append([i, j])
            d = torch.norm(coords[i] - coords[j], dim=0, keepdim=True)
            edge_attr.append(d)
    edge_index = torch.tensor(edge_idx, device=device).t().long() if edge_idx else torch.empty((2, 0), dtype=torch.long, device=device)
    edge_attr = torch.stack(edge_attr) if edge_attr else torch.empty((0, 1), device=device)

    data = Data(
        x=coords,
        edge_index=edge_index,
        edge_attr=edge_attr,
        demand=demands,
        capacity=torch.full((num_nodes,), capacity, device=device),
        batch=torch.zeros(num_nodes, dtype=torch.long, device=device),
    )
    data.num_graphs = 1
    return data


def make_batch(instances: List[Data]) -> Data:
    device = instances[0].x.device
    x_list, edge_index_list, edge_attr_list, demand_list, capacity_list, batch_vec = [], [], [], [], [], []
    offset = 0
    for b, d in enumerate(instances):
        n = d.x.size(0)
        x_list.append(d.x)
        demand_list.append(d.demand)
        capacity_list.append(d.capacity)
        batch_vec.append(torch.full((n,), b, dtype=torch.long, device=device))
        if d.edge_index.numel() > 0:
            edge_index_list.append(d.edge_index + offset)
            edge_attr_list.append(d.edge_attr)
        offset += n
    x = torch.cat(x_list, dim=0)
    demand = torch.cat(demand_list, dim=0)
    capacity = torch.cat(capacity_list, dim=0)
    batch = torch.cat(batch_vec, dim=0)
    if edge_index_list:
        edge_index = torch.cat(edge_index_list, dim=1)
        edge_attr = torch.cat(edge_attr_list, dim=0)
    else:
        edge_index = torch.empty((2, 0), dtype=torch.long, device=device)
        edge_attr = torch.empty((0, 1), device=device)
    data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, demand=demand, capacity=capacity, batch=batch)
    data.num_graphs = len(instances)
    return data


def naive_roundtrip_cost(coords_np: np.ndarray) -> float:
    depot = coords_np[0]
    dists = np.linalg.norm(coords_np[1:] - depot, axis=1)
    return float(2.0 * dists.sum())


def eval_greedy_baseline_on_val(customers: int, capacity: float, seed: int, instances: int, batch_size: int, device: torch.device) -> Tuple[float, float]:
    """Evaluate the GreedyGraphTransformerBaseline on the same validation seeds
    as the unified trainer (first val_count seeds starting from data_seed).
    Returns (avg_cost_per_customer, avg_time_seconds).
    """
    # Match pipeline's validation split logic
    val_count = max(32, min(128, instances // 5))
    seeds = [seed + i for i in range(val_count)]

    model = GreedyGraphTransformerBaseline().to(device)
    model.eval()
    n_nodes = customers + 1
    n_steps = customers + 6

    costs = []
    times = []
    with torch.no_grad():
        total_batches = (len(seeds) + batch_size - 1) // batch_size
        for bi, i in enumerate(range(0, len(seeds), batch_size), start=1):
            batch_seeds = seeds[i:i + batch_size]
            inst_list = [make_instance(n_nodes, capacity, device, s) for s in batch_seeds]
            batch_data = make_batch(inst_list)
            print(f"[eval] greedy_baseline batch {bi}/{total_batches} (val instances {i+1}-{i+len(batch_seeds)})")
            t0 = time.perf_counter()
            actions, _ = model(batch_data, n_steps=n_steps, greedy=True)
            t1 = time.perf_counter()
            c = euclidean_cost(batch_data.x, actions, batch_data)
            costs.append(c.detach().cpu())
            times.append(t1 - t0)
    cost_tensor = torch.cat(costs) if costs else torch.tensor([], dtype=torch.float32)
    avg_cost_per_customer = float(cost_tensor.mean().item()) / max(1, customers)
    avg_time = float(np.mean(times)) if times else float('nan')
    return avg_cost_per_customer, avg_time


def ensure_trained_and_load_summary(model: str, customers: int, instances: int, epochs: int, batch: int, capacity: float, lr: float, seed: int, base_out: Path, force: bool = False) -> Dict[str, float]:
    """Train via unified pipeline if summary cache is missing (or forced). Return summary dict."""
    device = torch.device('cpu')
    out_dir = base_out / f"cpu_{model}_C{customers}_I{instances}_E{epochs}_B{batch}"
    summary_path = out_dir / 'summary.json'
    if summary_path.exists() and not force:
        with summary_path.open('r') as f:
            print(f"[orchestrator] Reusing cached run: {summary_path}")
            return json.load(f)
    out_dir.mkdir(parents=True, exist_ok=True)
    # Train
    print(f"[orchestrator] Training {model} on CPU: C={customers} I={instances} E={epochs} B={batch} lr={lr} seed={seed}")
    t0 = time.perf_counter()
    summary = train_pipeline(
        model_name=model,
        device=device,
        use_amp=False,
        customers=customers,
        instances=instances,
        epochs=epochs,
        batch_size=batch,
        capacity=capacity,
        lr=lr,
        data_seed=seed,
        out_dir=out_dir,
    )
    t1 = time.perf_counter()
    print(f"[orchestrator] Finished training {model} in {t1 - t0:.2f}s; best val/cust={summary['best_val_cost_per_customer']:.4f}")
    enriched = {
        'model': model,
        'customers': customers,
        'instances': instances,
        'epochs': epochs,
        'batch': batch,
        'capacity': capacity,
        'lr': lr,
        'data_seed': seed,
        'best_val_cost_per_customer': summary['best_val_cost_per_customer'],
        'train_time_seconds': t1 - t0,
        'out_dir': str(out_dir),
    }
    with summary_path.open('w') as f:
        json.dump(enriched, f, indent=2)
    return enriched


def generate_plots(df: pd.DataFrame, out_plot: Path):
    out_plot.parent.mkdir(parents=True, exist_ok=True)
    plt.style.use('seaborn-v0_8')

    # Bar plot of Val/Cust for all models present
    order = ['dynamic_gt_rl', 'static_rl', 'pointer_rl', 'greedy_baseline', 'naive_baseline', 'gat_rl', 'gat_rl_legacy']
    df_plot = df.copy()
    df_plot['Model'] = pd.Categorical(df_plot['Model'], categories=order, ordered=True)
    df_plot = df_plot.sort_values('Model')

    plt.figure(figsize=(10, 6))
    plt.bar(df_plot['Model'], df_plot['Val/Cust'])
    plt.ylabel('Average Cost per Customer')
    plt.title('CPU Comparative Study')
    plt.xticks(rotation=30, ha='right')
    plt.tight_layout()
    plt.savefig(out_plot, dpi=300)
    plt.close()
    print(f"Saved plot to {out_plot}")


def main():
    parser = argparse.ArgumentParser(description='CPU Comparative Study Orchestrator (train missing, reuse cached, make plots)')
    parser.add_argument('--models', nargs='+', default=['dynamic_gt_rl', 'static_rl', 'greedy_baseline', 'naive_baseline'],
                        help='Models to include')
    parser.add_argument('--customers', type=int, default=20, help='Number of customers (excluding depot)')
    parser.add_argument('--instances', type=int, default=800, help='Instances for training (unified pipeline)')
    parser.add_argument('--epochs', type=int, default=20)
    parser.add_argument('--batch', type=int, default=32)
    parser.add_argument('--capacity', type=float, default=3.0)
    parser.add_argument('--lr', type=float, default=1e-4)
    parser.add_argument('--seed', type=int, default=12345, help='Base seed for deterministic instances')
    parser.add_argument('--recalculate_rl_weights', action='store_true', help='Force retrain RL models (ignore cache)')
    parser.add_argument('--train_out_dir', type=str, default='results_train', help='Where unified trainer stores runs')
    parser.add_argument('--out_csv', type=str, default='results/comparative_study_cpu.csv')
    parser.add_argument('--out_plot', type=str, default='utils/plots/comparative_study_results.png')
    parser.add_argument('--test_seed', type=int, default=20250809, help='Seed for single test instance application step')
    args = parser.parse_args()

    device = torch.device('cpu')
    print("[orchestrator] Starting comparative study")
    print(f"[orchestrator] Models: {' '.join(args.models)}")
    print(f"[orchestrator] Params: customers={args.customers} instances={args.instances} epochs={args.epochs} batch={args.batch} lr={args.lr} seed={args.seed}")

    # Collect rows
    rows: List[Dict[str, object]] = []
    base_out = project_root / args.train_out_dir

    # Baselines depend on validation set; evaluate once if requested
    if 'naive_baseline' in args.models or 'greedy_baseline' in args.models:
        val_count = max(32, min(128, args.instances // 5))
        seeds = [args.seed + i for i in range(val_count)]
        n_nodes = args.customers + 1
        # Prepare val instances
        val_graphs = [make_instance(n_nodes, args.capacity, device, s) for s in seeds]
        # Naive baseline
        if 'naive_baseline' in args.models:
            print("[orchestrator] Evaluating naive_baseline on deterministic validation set")
            naive_vals = []
            for g in val_graphs:
                coords = g.x.detach().cpu().numpy()
                naive_vals.append(naive_roundtrip_cost(coords) / max(1, args.customers))
            rows.append({
                'Model': 'naive_baseline',
                'Val/Cust': float(np.mean(naive_vals)),
                'CPU Time (s)': None,
                'OutDir': None,
            })
        # Greedy baseline
        if 'greedy_baseline' in args.models:
            print("[orchestrator] Evaluating greedy_baseline on deterministic validation set")
            avg_cost, avg_time = eval_greedy_baseline_on_val(args.customers, args.capacity, args.seed, args.instances, args.batch, device)
            rows.append({
                'Model': 'greedy_baseline',
                'Val/Cust': float(avg_cost),
                'CPU Time (s)': float(avg_time),
                'OutDir': None,
            })

    # RL models via unified trainer
    for model in ['dynamic_gt_rl', 'static_rl', 'pointer_rl']:
        if model not in args.models:
            continue
        summary = ensure_trained_and_load_summary(
            model=model,
            customers=args.customers,
            instances=args.instances,
            epochs=args.epochs,
            batch=args.batch,
            capacity=args.capacity,
            lr=args.lr,
            seed=args.seed,
            base_out=base_out,
            force=args.recalculate_rl_weights,
        )
        rows.append({
            'Model': model,
            'Val/Cust': float(summary['best_val_cost_per_customer']),
            'CPU Time (s)': float(summary['train_time_seconds']),
            'OutDir': summary['out_dir'],
        })

    # TODO: Optional integration for 'gat_rl' and 'gat_rl_legacy' via ../GAT_RL
    # Placeholder: skip unless explicitly implemented.

    df = pd.DataFrame(rows)
    out_csv = project_root / args.out_csv
    out_csv.parent.mkdir(parents=True, exist_ok=True)
    df.to_csv(out_csv, index=False)
    print(f"Saved results table to {out_csv}")

    out_plot = project_root / args.out_plot
    generate_plots(df, out_plot)

    # Apply all requested models (except naive_baseline) to a single deterministic test instance and save separate route plots

    # Helpers: turn raw actions into a feasible CVRP route and validate
    def actions_to_indices(actions):
            act = actions.squeeze()
            if act.dim() > 1:
                act = act[:, 0]
            return [int(i) for i in act.tolist()]

    def build_feasible_route(idx_seq, demands_tensor, capacity_val):
            d = demands_tensor.view(-1).detach().cpu().numpy()
            n = d.shape[0]
            served = set()
            route = [0]
            rem = float(capacity_val)
            for idx in idx_seq:
                if idx == 0:
                    if route[-1] != 0:
                        route.append(0)
                        rem = float(capacity_val)
                    continue
                if idx <= 0 or idx >= n:
                    continue
                if idx in served:
                    # skip duplicates
                    continue
                dem = float(d[idx])
                if dem <= rem:
                    route.append(idx)
                    served.add(idx)
                    rem -= dem
                else:
                    # return to depot, then serve if fits
                    if route[-1] != 0:
                        route.append(0)
                    rem = float(capacity_val)
                    if dem <= rem:
                        route.append(idx)
                        served.add(idx)
                        rem -= dem
            # Serve any remaining customers by greedy fill with depot resets
            remaining = [i for i in range(1, n) if i not in served]
            for idx in remaining:
                dem = float(d[idx])
                if route[-1] != 0:
                    route.append(0)
                rem = float(capacity_val)
                if dem <= rem:
                    route.append(idx)
                    served.add(idx)
                    rem -= dem
            if route[-1] != 0:
                route.append(0)
            # compress consecutive depots
            comp = [route[0]]
            for x in route[1:]:
                if not (x == 0 and comp[-1] == 0):
                    comp.append(x)
            return comp

    def validate_route(route, n_customers):
            # route includes depots 0 between tours; ensure all 1..n served exactly once
            seen = [i for i in route if i != 0]
            ok_once = len(seen) == len(set(seen)) == n_customers
            no_dd = all(not (route[i] == 0 and route[i+1] == 0) for i in range(len(route)-1))
            return ok_once and no_dd
    
    try:
        from src.models import StaticRLGraphTransformer, DynamicGraphTransformerModel, PointerRLModel
        from src.utils.RL.euclidean_cost import euclidean_cost
        from torch_geometric.data import Data

        print("[orchestrator] Applying trained models to a test instance and saving route plots")
        # Build deterministic test instance
        customers = args.customers
        n = customers + 1
        device = torch.device('cpu')
        g = torch.Generator(device=device); g.manual_seed(args.test_seed)
        coords = (torch.randint(1, 101, (n, 2), generator=g, device=device, dtype=torch.int64).float() / 100.0)
        demands = (torch.randint(1, 11, (n, 1), generator=g, device=device, dtype=torch.int64).float() / 10.0)
        demands[0] = 0.0
        edge_idx = []; edge_attr = []
        for i in range(n):
            for j in range(n):
                if i == j: continue
                edge_idx.append([i, j])
                edge_attr.append(torch.norm(coords[i] - coords[j]).unsqueeze(0))
        edge_index = torch.tensor(edge_idx, device=device).t().long()
        edge_attr = torch.stack(edge_attr)
        inst = Data(x=coords, edge_index=edge_index, edge_attr=edge_attr,
                    demand=demands, capacity=torch.full((n,), args.capacity, device=device),
                    batch=torch.zeros(n, dtype=torch.long, device=device))
        inst.num_graphs = 1

        def eval_model(m):
            m.eval()
            with torch.no_grad():
                actions, _ = m(inst, n_steps=customers + 6, greedy=True)
            cost = euclidean_cost(inst.x, actions, inst)[0].item() / customers
            return actions, float(cost)

        # Prepare output dir
        plots_dir = project_root / 'utils' / 'plots'
        plots_dir.mkdir(parents=True, exist_ok=True)

        # Map model name -> constructor and checkpoint path
        model_specs = {
            'dynamic_gt_rl': (DynamicGraphTransformerModel, project_root / args.train_out_dir / f"cpu_dynamic_gt_rl_C{args.customers}_I{args.instances}_E{args.epochs}_B{args.batch}" / 'checkpoint.pt'),
            'static_rl': (StaticRLGraphTransformer, project_root / args.train_out_dir / f"cpu_static_rl_C{args.customers}_I{args.instances}_E{args.epochs}_B{args.batch}" / 'checkpoint.pt'),
            'pointer_rl': (PointerRLModel, project_root / args.train_out_dir / f"cpu_pointer_rl_C{args.customers}_I{args.instances}_E{args.epochs}_B{args.batch}" / 'checkpoint.pt'),
            'greedy_baseline': (GreedyGraphTransformerBaseline, None),
        }

        for name in args.models:
            if name == 'naive_baseline':
                continue
            if name not in model_specs:
                continue
            ctor, ckpt_path = model_specs[name]
            model = ctor()
            # Load checkpoint if available
            if ckpt_path is not None and ckpt_path.exists():
                try:
                    state = torch.load(ckpt_path, map_location='cpu')
                    state = state.get('model_state_dict', state)
                    model.load_state_dict(state, strict=False)
                    print(f"[apply] Loaded checkpoint for {name}: {ckpt_path}")
                except Exception as e:
                    print(f"[apply] WARNING: failed to load checkpoint for {name}: {e}")

            actions, val_cust = eval_model(model)
            # Construct feasible route and plot
            idx_seq = actions_to_indices(actions)
            route = build_feasible_route(idx_seq, demands, args.capacity)
            ok = validate_route(route, customers)
            if not ok:
                print(f'[apply] WARNING: route validation failed for {name}; repaired route used')
            c = coords.detach().cpu().numpy()
            fig, ax = plt.subplots(figsize=(5, 5))
            ax.scatter(c[1:, 0], c[1:, 1], c='blue', s=20, label='Customers')
            ax.scatter(c[0:1, 0], c[0:1, 1], c='red', s=50, label='Depot')
            for i in range(len(route) - 1):
                a, b = route[i], route[i + 1]
                ax.plot([c[a, 0], c[b, 0]], [c[a, 1], c[b, 1]], 'g-', alpha=0.8)
            ax.set_title(f'{name}  (cost/customer={val_cust:.3f})')
            ax.legend()
            fig.tight_layout()
            out_png = plots_dir / f'test_instance_route_{name}.png'
            fig.savefig(out_png, dpi=200)
            plt.close(fig)
            print(f"[apply] Saved {out_png}")
            # Save route-only JSON
            out_json = plots_dir / f'test_instance_route_{name}.json'
            with out_json.open('w') as f:
                import json as _json
                _json.dump(route, f)
            print(f"[apply] Saved {out_json}")
    except Exception as e:
        print(f"[apply] WARNING: failed to apply models to test instance: {e}")

    # Refresh final plots with annotated styling (lightblue+rims, (i,d) labels, size by demand)
    try:
        print("[apply] Refreshing final plots with annotated styling")
        # Deterministic test instance
        customers = args.customers
        n = customers + 1
        device = torch.device('cpu')
        g = torch.Generator(device=device); g.manual_seed(args.test_seed)
        coords = (torch.randint(1, 101, (n, 2), generator=g, device=device, dtype=torch.int64).float() / 100.0)
        demands = (torch.randint(1, 11, (n, 1), generator=g, device=device, dtype=torch.int64).float() / 10.0)
        demands[0] = 0.0
        edge_idx = []; edge_attr = []
        for i in range(n):
            for j in range(n):
                if i == j: continue
                edge_idx.append([i, j])
                edge_attr.append(torch.norm(coords[i] - coords[j]).unsqueeze(0))
        edge_index = torch.tensor(edge_idx, device=device).t().long()
        edge_attr = torch.stack(edge_attr)
        inst = Data(x=coords, edge_index=edge_index, edge_attr=edge_attr,
                    demand=demands, capacity=torch.full((n,), args.capacity, device=device),
                    batch=torch.zeros(n, dtype=torch.long, device=device))
        inst.num_graphs = 1

        def to_idx_seq(actions):
            act = actions.squeeze()
            if act.dim() > 1:
                act = act[:, 0]
            return [int(i) for i in act.tolist()]

        def build_feasible_route(idx_seq, demands_tensor, capacity_val):
            d = demands_tensor.view(-1).tolist()
            N = len(d)
            served = set()
            route = [0]
            rem = float(capacity_val)
            for idx in idx_seq:
                if idx == 0:
                    if route[-1] != 0:
                        route.append(0)
                        rem = float(capacity_val)
                    continue
                if idx <= 0 or idx >= N or idx in served:
                    continue
                dem = float(d[idx])
                if dem <= rem:
                    route.append(idx)
                    served.add(idx)
                    rem -= dem
                else:
                    if route[-1] != 0:
                        route.append(0)
                    rem = float(capacity_val)
                    if dem <= rem:
                        route.append(idx)
                        served.add(idx)
                        rem -= dem
            for idx in range(1, N):
                if idx in served:
                    continue
                dem = float(d[idx])
                if route[-1] != 0:
                    route.append(0)
                rem = float(capacity_val)
                if dem <= rem:
                    route.append(idx)
                    served.add(idx)
                    rem -= dem
            if route[-1] != 0:
                route.append(0)
            comp = [route[0]]
            for x in route[1:]:
                if not (x == 0 and comp[-1] == 0):
                    comp.append(x)
            return comp

        plots_dir = project_root / 'utils' / 'plots'
        c = coords.detach().cpu().numpy()
        dvals = demands.detach().cpu().numpy().reshape(-1)
        base, scale = 40.0, 300.0
        sizes = base + scale * dvals

        # Build models again to ensure consistency (no retrain)
        from src.models import StaticRLGraphTransformer, DynamicGraphTransformerModel
        spec = {
            'dynamic_gt_rl': DynamicGraphTransformerModel,
            'static_rl': StaticRLGraphTransformer,
            'greedy_baseline': GreedyGraphTransformerBaseline,
        }
        for name in args.models:
            if name == 'naive_baseline' or name not in spec:
                continue
            model = spec[name]()
            # Load checkpoint if available
            ckpt_path = project_root / args.train_out_dir / f"cpu_{name}_C{args.customers}_I{args.instances}_E{args.epochs}_B{args.batch}" / 'checkpoint.pt'
            if ckpt_path.exists():
                try:
                    state = torch.load(ckpt_path, map_location='cpu')
                    state = state.get('model_state_dict', state)
                    model.load_state_dict(state, strict=False)
                except Exception:
                    pass
            model.eval()
            with torch.no_grad():
                actions, _ = model(inst, n_steps=customers + 6, greedy=True)
            route = build_feasible_route(to_idx_seq(actions), demands, args.capacity)
            with torch.no_grad():
                actions2, _ = model(inst, n_steps=customers + 6, greedy=True)
                cost_pc = float(euclidean_cost(inst.x, actions2, inst)[0].item()) / customers
            fig, ax = plt.subplots(figsize=(6, 6))
            for i in range(len(route) - 1):
                a, b = route[i], route[i + 1]
                ax.plot([c[a, 0], c[b, 0]], [c[a, 1], c[b, 1]], '-', color='g', alpha=0.85, linewidth=1.6)
            ax.scatter(c[0:1, 0], c[0:1, 1], marker='*', s=180, c='red', edgecolors='black', linewidths=1.0, label='Depot')
            ax.annotate('(0,0.0)', (c[0, 0], c[0, 1]), textcoords='offset points', xytext=(6, 6), fontsize=9, color='black')
            ax.scatter(c[1:, 0], c[1:, 1], s=sizes[1:], c='lightblue', edgecolors='black', linewidths=0.9, label='Customers')
            for i in range(1, n):
                ax.annotate(f'({i},{dvals[i]:.1f})', (c[i, 0], c[i, 1]), textcoords='offset points', xytext=(6, 6), fontsize=8, color='black')
            ax.set_title(f'{name}  (cost/customer={cost_pc:.3f})')
            ax.set_aspect('equal', adjustable='box')
            ax.legend(loc='upper right')
            fig.tight_layout()
            out_png = plots_dir / f'test_instance_route_{name}.png'
            fig.savefig(out_png, dpi=200)
            plt.close(fig)
            # Save route-only JSON
            (plots_dir / f'test_instance_route_{name}.json').write_text(json.dumps(route))
            print(f"[apply] Refreshed {out_png}")
    except Exception as e:
        print(f"[apply] WARNING: failed to refresh annotated plots: {e}")


if __name__ == '__main__':
    main()