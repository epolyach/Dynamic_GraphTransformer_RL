# Default configuration loaded first and deep-merged with any provided config
# Put all stable/tuning parameters here. Per-experiment configs override only what they study.

# Working directory (will usually be overridden by scale-specific config)
working_dir_path: "results/default"

# =============================
# Core CVRP problem parameters
# =============================
problem:
  num_customers: 20         # Default customers (excluding depot)
  vehicle_capacity: 30      # Integer capacity
  demand_range: [1, 10]     # Integer demands [min, max]
  coord_range: 100          # Coordinates 0..coord_range, then normalized to [0,1]

# =============================
# Training parameters
# =============================
training:
  num_batches_per_epoch: 1500 # Paper's scale: 768,000 instances per epoch
  batch_size: 512            # Default batch size (paper's value)
  num_epochs: 100            # Default number of epochs
  learning_rate: 1e-4        # Base LR
  validation_frequency: 2    # Run validation every N epochs

# =============================
# Rollout baseline (optional)
# =============================
baseline:
  type: "rollout"
  eval_batches: 5         # Slight reduction from 5
  update:
    enabled: true
    frequency: 1          # Check every 1 epochs instead of 4
    significance_test: true  
    p_value: 0.10         # More lenient from 0.05


# =============================
# Experiment control
# =============================
experiment:
  random_seed: 42
  device: "cpu"
  use_advanced_features: true
  strict_validation: true

# =============================
# Model architecture - Graph Transformer (GT/DGT)
# =============================
model:
  input_dim: 3                 # Node feature dimension
  hidden_dim: 256               # Hidden dimension for transformers
  num_heads: 4                  # Number of attention heads
  num_layers: 4                 # Number of transformer layers
  transformer_dropout: 0.1      # Dropout rate for transformers
  feedforward_multiplier: 2     # Feedforward expansion factor

# =============================
# Model architecture - GAT specific
# =============================
model_gat:
  # GAT uses different architecture parameters
  gat_hidden_dim: 256          # Hidden dimension for GAT (can differ from transformer)
  gat_edge_dim: 16             # Edge feature dimension for GAT
  gat_layers: 4                # Number of GAT layers
  gat_dropout: 0.6             # Higher dropout for GAT (prevents overfitting)
  gat_negative_slope: 0.2      # LeakyReLU slope for GAT attention


# =============================
# Advanced training parameters - Graph Transformer (GT/DGT)
# =============================
training_advanced:
  # Temperature scheduling for exploration
  use_adaptive_temperature: true
  temp_start: 2.5              # Initial temperature
  temp_min: 0.15               # Final temperature
  temp_adaptation_rate: 0.18   # Temperature decay rate
  
  # Entropy regularization
  entropy_coef: 0.03           # Entropy coefficient
  entropy_min: 0.002           # Minimum entropy

  # Optimizer settings
  weight_decay: 0.0001         # L2 regularization
  gradient_clip_norm: 2.0      # Gradient clipping
  
  # Learning rate scheduling  
  use_lr_scheduling: true
  scheduler_type: "cosine"     # Cosine annealing
  min_lr: 0.000001             # Minimum learning rate
  
  # Early stopping
  use_early_stopping: true
  early_stopping_patience: 10
  early_stopping_delta: 0.0001

# =============================
# Advanced training parameters - GAT specific
# =============================
# These override training_advanced when training GAT models
gat_training:
  learning_rate: 0.0002        # Higher LR for GAT (overrides training.learning_rate)
  temp_start: 2.5              # Different temperature schedule
  temp_min: 0.3                # Higher minimum temperature
  temp_adaptation_rate: 0.12   # Slower temperature adaptation
  entropy_coef: 0.08           # Higher entropy for GAT exploration
  entropy_min: 0.008           # Higher entropy minimum
  gradient_clip_norm: 1.5      # Tighter gradient clipping for GAT stability
  early_stopping_patience: 15  # More patience for GAT convergence
  use_adaptive_temperature: false  # Disable adaptive temperature for GAT (moved from code)
  baseline:
    eval_batches: 5   
    update:
      enabled: true
      frequency: 1
      significance_test: true
      p_value: 0.10
      warmup_epochs: 5

# =============================
# Inference and evaluation
# =============================
inference:
  default_temperature: 1.0
  max_steps_multiplier: 2


# =============================
# Logging
# =============================
logging:
  level: "INFO"
  format: "%(message)s"

# =============================
# Benchmark settings
# =============================
benchmark:
  scaling:
    distance_scale: 100000  # Used by GPU/CPU internals for integer distance scaling (results scaled back)

# =============================
# GPU Configuration
# =============================
gpu:
  enabled: true                        # Enable GPU training if available
  device: "cuda:0"                     # GPU device to use (cuda:0, cuda:1, etc.)
  mixed_precision: true                # Enable automatic mixed precision (FP16)
  memory_fraction: 0.95                # Fraction of GPU memory to use
  pin_memory: true                     # Pin memory for faster CPU-GPU transfer
  non_blocking: true                   # Non-blocking tensor transfers
  gradient_accumulation_steps: 1       # Gradient accumulation for larger effective batch size
  num_workers: 4                       # Data loading workers
  prefetch_factor: 2                   # Batches to prefetch

# =============================
# Memory optimization
# =============================
memory:
  clear_cache_interval: 5              # Clear GPU cache every N epochs
  memory_check_interval: 50            # Check memory usage every N batches
  memory_warning_threshold: 0.9        # Warn if memory usage exceeds this fraction
