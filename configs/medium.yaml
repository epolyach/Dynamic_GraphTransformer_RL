# Medium-scale configuration for thorough evaluation
# Balanced between development speed and comprehensive results

# =====================================================================
# CRITICAL PARAMETERS - FREQUENTLY CHANGED (Business Logic)
# =====================================================================

# Problem Definition - Core CVRP problem settings (medium scale)
problem:
  num_customers: 50         # Medium-scale customer count
  vehicle_capacity: 40      # Increased capacity for larger problems
  demand_range: [1, 10]     # Integer demands [min, max]
  coord_range: 100          # Coordinate range [0, coord_range] then normalized to unit square

# Training Configuration - Medium-scale training parameters
training:
  num_instances: 4096       # More training instances for better convergence
  batch_size: 32            # Larger batch size for stability
  num_epochs: 64            # Moderate epoch count for medium-scale
  learning_rate: 5e-4       # Slightly lower LR for larger problems
  train_val_split: 0.8      # Training/validation split ratio
  validation_frequency: 3   # Run validation every N epochs
  
# Experimental Control - Critical for reproducibility
experiment:
  random_seed: 42           # Random seed for reproducible results
  device: "cpu"             # Device to use ("cpu" or "cuda")
  
# =====================================================================
# MODERATE PARAMETERS - MODEL ARCHITECTURE & ADVANCED TRAINING
# =====================================================================

# Model Architecture - Enhanced for medium-scale problems
model:
  # Core architecture parameters
  input_dim: 3              # Input dimension (coords + demand)
  hidden_dim: 128           # Increased hidden dimension for medium scale
  num_heads: 8              # More attention heads for complex patterns  
  num_layers: 4             # Deeper transformer layers
  transformer_dropout: 0.1  # Dropout for transformer encoder layers
  feedforward_multiplier: 2 # Transformer feedforward dim multiplier (feedforward = hidden_dim * multiplier)
  edge_embedding_divisor: 4 # Edge embedding divisor (edge_dim = hidden_dim // divisor)
  
  # Model-specific parameters
  pointer_network:
    input_multiplier: 2     # Input dim multiplier for pointer network layers (hidden_dim * multiplier)
    context_multiplier: 2   # Context multiplier for pointer network layers (hidden_dim * multiplier)
    output_dim: 1           # Output dimension for pointer network final layer
  
  dynamic_graph_transformer:
    state_features: 4       # Number of dynamic state features
    residual_gate_init: -2.19722458  # Initial value for learnable residual gate
    update_input_multiplier: 2  # Input multiplier for dynamic update layers
    update_activation: "relu"   # Activation function for dynamic updates
    
  graph_transformer:
    input_multiplier: 2     # Input dim multiplier for graph transformer pointer layers
    output_dim: 1           # Output dimension for graph transformer pointer layers
    
  graph_attention_transformer:
    input_multiplier: 2     # Input dim multiplier for GAT pointer layers
    output_dim: 1           # Output dimension for GAT pointer layers
    
  legacy_gat:
    node_input_dim: 3       # Node input dimension for legacy GAT
    edge_input_dim: 1       # Edge input dimension for legacy GAT  
    hidden_dim: 128         # Hidden dimension (legacy GAT uses different size)
    edge_dim: 16           # Edge embedding dimension
    num_layers: 4           # Number of GAT layers
    negative_slope: 0.2     # LeakyReLU negative slope
    dropout: 0.5           # Reduced dropout for medium scale

# Advanced Training Configuration
training_advanced:
  # Optimization parameters
  optimizer: "Adam"         # Optimizer type
  gradient_clip_norm: 1.0   # Gradient clipping for stability
  
  # Learning rate schedule (more conservative for medium scale)
  warmup_epochs: 10         # Longer warmup for medium scale
  min_lr: 1e-5              # Lower minimum LR
    
  # Reinforcement Learning parameters
  entropy_coef: 0.02        # Higher entropy for exploration in medium scale
  entropy_min: 0.001        # Small minimum entropy to maintain exploration
  temp_start: 2.0           # Higher initial temperature for medium scale
  temp_min: 0.3             # Higher minimum temperature
  
  # Legacy GAT training parameters
  legacy_gat:
    learning_rate: 5e-5     # Lower learning rate for stable training
    temperature: 2.5        # Temperature parameter for legacy training
    
# =====================================================================
# ACCEPTABLE PARAMETERS - SYSTEM & INFRASTRUCTURE
# =====================================================================

# System Configuration - Hardware and performance settings
system:
  # CPU Threading (only used when device="cpu")
  cpu_optimization:
    auto_detect_threads: true    # Auto-detect number of CPU threads
    max_threads: null           # Override max threads (null = auto-detect)
    inter_op_threads_divisor: 4  # Divisor for inter-op threads (max_threads // divisor)
    
  # Environment variables for CPU optimization
  openmp_settings:
    kmp_blocktime: "0"          # KMP_BLOCKTIME
    kmp_settings: "0"           # KMP_SETTINGS (disable verbose)
    kmp_affinity: "granularity=fine,compact,1,0"  # KMP_AFFINITY

# Inference and Evaluation Settings
inference:
  default_temperature: 1.0      # Default temperature for model forward passes
  greedy_evaluation: true       # Use greedy decoding for validation
  max_steps_multiplier: 2       # Max steps = num_customers * multiplier
  attention_temperature_scaling: 0.5  # Attention scaling factor (sqrt power for attention)
  
  # Numerical stability
  log_prob_epsilon: 1e-12       # Small epsilon for log probability computation
  masked_score_value: -1e9      # Large negative value for masked scores

# Output and Logging Configuration
output:
  # Directory structure
  results_base_dir: "results"   # Base directory for all results
  subdirectories:
    pytorch: "pytorch"          # Model checkpoints and state dicts
    analysis: "analysis"        # Analysis results and study data
    logs: "logs"               # Training logs and metrics
    csv: "csv"                 # CSV export files
    checkpoints: "checkpoints"  # Training checkpoints
    
  # File naming patterns
  model_filename_template: "model_{model_name}.pt"  # Template for model files
  
  # Legacy paths
  legacy_paths:
    checkpoints: "legacy_checkpoints"  # Legacy GAT checkpoint subdirectory
    training_logs: "logs/training"     # Legacy training CSV logs
    checkpoint_name: "actor.pt"        # Legacy checkpoint filename
    
# Logging Configuration
logging:
  level: "INFO"                 # Logging level
  format: "%(message)s"        # Log message format
  progress_bars: true          # Enable tqdm progress bars
