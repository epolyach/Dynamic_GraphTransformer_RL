# GAT-optimized configuration for better convergence
# Based on tiny.yaml but with GAT-specific adjustments

working_dir_path: "results/gat_optimized"

problem:
  num_customers: 7

training:
  num_batches_per_epoch: 5
  batch_size: 128
  num_epochs: 80
  learning_rate: 0.001  # Higher for GAT (was 0.0003) - helps with gradient flow
  validation_frequency: 4

baseline:
  type: "rollout"  # or "mean"
  eval_batches: 3
  update:
    frequency: 4  # Update baseline every 4 epochs
    significance_test: false  # Disable statistical test for GAT
    p_value: 0.10

training_advanced:
  use_early_stopping: true
  early_stopping_patience: 30  # More patience for GAT (was 25)
  early_stopping_delta: 0.0001
  
  # Gentler temperature schedule for GAT (which already has internal scaling issues)
  temp_start: 2.0  # Reduced from 5.0 - less initial randomness
  temp_min: 0.5    # Raised from 0.2 - avoid being too greedy
  temp_adaptation_rate: 0.05  # Slower adaptation (was 0.15)
  
  # Different entropy regularization for GAT
  entropy_coef: 0.01  # Lower than default 0.05 - GAT needs less entropy
  entropy_min: 0.001  # Minimum entropy coefficient
  
  # Proper gradient clipping
  gradient_clip_norm: 1.0  # Reduced from 2.0 for stability
  
  # Learning rate scheduling
  use_lr_scheduling: true
  scheduler_type: "cosine"  # or "plateau"
  min_lr: 0.00001
  lr_patience: 10
  
  # Weight decay for regularization
  weight_decay: 0.0001

model:
  input_dim: 3
  hidden_dim: 128
  num_heads: 4       # For compatibility (GAT uses 8 heads internally for pointer)
  num_layers: 3      # Doesn't affect GAT but kept for consistency
  transformer_dropout: 0.1
  feedforward_multiplier: 2

inference:
  default_temperature: 1.0
  max_steps_multiplier: 2
  
experiment:
  random_seed: 42
  use_advanced_features: true

data:
  generator_type: "uniform"
  coord_range: [0, 1]
  demand_range: [0.01, 0.25]
  capacity: 1.0

logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
