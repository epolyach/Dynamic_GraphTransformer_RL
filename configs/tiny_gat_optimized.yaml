# GAT-optimized configuration for tiny experiments
# Based on tiny.yaml but with GAT-specific adjustments for better loss visibility

working_dir_path: "training_cpu/results/tiny_gat"

problem:
  num_customers: 7

training:
  num_batches_per_epoch: 5
  batch_size: 128
  num_epochs: 80
  learning_rate: 0.001  # Higher for GAT (was 0.0003) - helps with gradient flow and loss visibility
  validation_frequency: 4

baseline:
  type: "rollout"
  eval_batches: 3
  update:
    frequency: 2  # More frequent updates
    significance_test: false  # Disable for GAT
    p_value: 0.10

training_advanced:
  use_early_stopping: true
  early_stopping_patience: 30
  early_stopping_delta: 0.0001
  
  # Gentler temperature schedule for GAT
  temp_start: 2.0  # Reduced from 5.0
  temp_min: 0.5    # Raised from 0.2
  temp_adaptation_rate: 0.05  # Slower adaptation
  
  # Higher entropy for more visible loss
  entropy_coef: 0.1   # Increased to make entropy loss more visible
  entropy_min: 0.01   # Higher minimum
  
  # Gradient clipping
  gradient_clip_norm: 1.0  # Reduced for stability
  
  # Learning rate scheduling
  use_lr_scheduling: true
  scheduler_type: "cosine"
  min_lr: 0.00001
  lr_patience: 10
  
  # Weight decay
  weight_decay: 0.0001

model:
  input_dim: 3
  hidden_dim: 128
  num_heads: 4
  num_layers: 3
  transformer_dropout: 0.1
  feedforward_multiplier: 2
  # GAT-specific parameters
  gat_edge_dim: 16
  gat_dropout: 0.6
  gat_negative_slope: 0.2

inference:
  default_temperature: 1.0
  max_steps_multiplier: 2
  
experiment:
  random_seed: 42
  use_advanced_features: true

logging:
  level: "INFO"
  format: "%(message)s"
