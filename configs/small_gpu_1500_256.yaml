# Small GPU config - Large batch size and many steps for GPU training
# Batch size 512, 1500 steps per epoch
working_dir_path: "../results/small_gpu_1500_256"

problem:
  num_customers: 10
  vehicle_capacity: 30

training:
  learning_rate: 0.0002
  num_batches_per_epoch: 1500   # 150 steps per epoch
  batch_size: 512              # Large batch size for GPU
  num_epochs: 100              # Default number of epochs

# Smaller model for faster training
model:
  hidden_dim: 256              # Half of default 256
  num_heads: 4
  num_layers: 4                # Reduced from default 4

# Baseline configuration for efficient training
baseline:
  eval_batches: 5              # Small eval dataset
  update:
    frequency: 2               # Update every 3 epochs
    warmup_epochs: 0

# Optional: specify inference settings
inference:
  max_steps_multiplier: 10
  default_temperature: 2.5

