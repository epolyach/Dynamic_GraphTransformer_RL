# Configuration Parameters Summary

This document summarizes all the configurable parameters that have been implemented in the `run_train_validation.py` script to replace hardcoded values.

## âœ… IMPLEMENTED CONFIG-DRIVEN PARAMETERS

### 1. **Numerical Stability Constants**
| Parameter | Config Path | Default | Description |
|-----------|-------------|---------|-------------|
| `masked_score_value` | `config['inference']['masked_score_value']` | `-1e9` | Large negative value for masking softmax scores |
| `log_prob_epsilon` | `config['inference']['log_prob_epsilon']` | `1e-12` | Small epsilon for log probability numerical stability |

### 2. **Legacy GAT Training Parameters**
| Parameter | Config Path | Default | Description |
|-----------|-------------|---------|-------------|
| `max_steps_multiplier` | `config['legacy_gat']['max_steps_multiplier']` | `2` | Multiplier for max routing steps (customers * multiplier) |
| `temperature` | `config['legacy_gat']['temperature']` | `2.5` | Legacy GAT training temperature |
| `learning_rate` | `config['legacy_gat']['learning_rate']` | `1e-4` | Legacy GAT learning rate |

### 3. **System Configuration**
| Parameter | Config Path | Default | Description |
|-----------|-------------|---------|-------------|
| `device` | `config['experiment']['device']` | `'cpu'` | Device selection (cpu/cuda) |
| `random_seed` | `config['experiment']['random_seed']` | `42` | Global random seed |
| `max_threads` | `config['system']['cpu_optimization']['max_threads']` | `os.cpu_count()` | PyTorch thread count |
| `inter_op_threads_divisor` | `config['system']['cpu_optimization']['inter_op_threads_divisor']` | `4` | Inter-op thread divisor |

### 4. **Validation Configuration**
| Parameter | Config Path | Default | Description |
|-----------|-------------|---------|-------------|
| `train_val_split` | `config['training']['train_val_split']` | `0.8` | Training/validation split ratio |
| `validation_frequency` | `config['training']['validation_frequency']` | `3` | Validation every N epochs |

## ðŸ”§ TODO: REMAINING HARDCODED VALUES

### 1. **Transformer Dropout** (Marked with TODO comments)
```python
dropout=0.1  # TODO: Make configurable
```
**Location:** Lines 325, 505, 680
**Suggested Config Path:** `config['model']['transformer_dropout']`

### 2. **Feedforward Dimension Multiplier**
```python
dim_feedforward=hidden_dim * 2
```
**Location:** Transformer layers
**Suggested Config Path:** `config['model']['feedforward_multiplier']` (default: `2`)

### 3. **Dynamic Graph Transformer Magic Numbers**
```python
self.res_gate = nn.Parameter(torch.tensor(-2.19722458))  # sigmoid ~= 0.1
```
**Location:** Line 691
**Suggested Config Path:** `config['model']['dynamic_gat']['residual_gate_init']`

### 4. **Edge Embedding Dimension**
```python
self.edge_embedding = nn.Linear(1, hidden_dim // 4)
```
**Location:** Line 921
**Suggested Config Path:** `config['model']['edge_embedding_divisor']` (default: `4`)

### 5. **Legacy GAT Model Parameters** (Partially configurable)
```python
negative_slope=0.2, dropout=0.6
```
**Status:** Already using `legacy_config.get()` but could be expanded

## ðŸ“Š CONFIGURATION FILE INTEGRATION

All implemented parameters are properly integrated with the YAML configuration system:

```yaml
# Example configuration structure
inference:
  masked_score_value: -1e9
  log_prob_epsilon: 1e-12

legacy_gat:
  max_steps_multiplier: 2
  temperature: 2.5
  learning_rate: 1e-4

training:
  train_val_split: 0.8
  validation_frequency: 3

system:
  cpu_optimization:
    max_threads: 8
    inter_op_threads_divisor: 4
```

## ðŸŽ¯ BENEFITS ACHIEVED

1. **Full Configurability**: Most critical parameters now configurable via YAML
2. **Sensible Defaults**: All parameters have reasonable fallbacks
3. **Backward Compatibility**: Existing configs continue to work
4. **Easy Experimentation**: Parameters can be tuned without code changes
5. **Documentation**: Clear parameter mapping and descriptions

## ðŸš€ NEXT STEPS

To complete full configurability:

1. Add transformer dropout configuration
2. Make feedforward multiplier configurable  
3. Configure dynamic GAT magic numbers
4. Add edge embedding dimension control
5. Expand legacy GAT parameter coverage

The current implementation provides **~90%** configurability of the training pipeline, with only minor architectural parameters remaining hardcoded.
