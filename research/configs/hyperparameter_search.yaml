
# Aggressive Hyperparameter Search Configuration for <0.5 Performance Target
# Based on temperature experiment insights: GAT+RL best performer, aggressive exploration needed

problem:
  size: 20
  capacity: 30

training:
  epochs: 128  # Increased for better convergence
  batch_size: 512
  train_instances: 4096  # More training data
  validation_instances: 2048  # More validation data
  learning_rate: 1e-4
  dropout: 0.1
  temperature:
    initial: 10.0
    min: 0.1
    decay: 0.99

# Focus on best performers from temperature experiments
models:
  - GAT+RL
  - DGT-Super+RL
  - DGT-Ultra+RL
  - DGT+RL  # Include full DGT for completeness

# Focused hyperparameter search space targeting <0.5 performance breakthrough
# Based on temperature results: GAT+RL best performer, focus search around promising regions
hyperparameters:
  # Focus on larger embedding dimensions that could break through
  embedding_dim: [256, 512, 1024]
  
  # Test deeper networks - more layers for complex pattern learning
  n_layers: [6, 8, 12]
  
  # Higher attention heads for better representational capacity
  n_heads: [8, 12, 16]
  
  # Learning rate focused around optimal ranges from literature
  learning_rate: [1e-3, 5e-4, 1e-4]
  
  # Dropout focused on regularization sweet spot
  dropout: [0.1, 0.2]
  
  # Batch sizes for stable gradient computation
  batch_size: [512, 1024]
  
  # Temperature regimes based on best temperature experiment results
  temp_start: [5.0, 7.0]  # Aggressive exploration like best temperature results
  temp_min: [0.02, 0.05]   # Low exploitation
  temp_decay: [0.15, 0.2]  # Sustained exploration

