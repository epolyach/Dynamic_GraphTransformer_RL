experiment:
  output_root: results/gradient_search
  random_seed: 42
inference:
  attention_temperature_scaling: 0.5
  default_temperature: 1.0
  greedy_evaluation: true
  log_prob_epsilon: 1e-12
  masked_score_value: -1e9
  max_steps_multiplier: 2
logging:
  checkpoint_every: 16
  format: '%(message)s'
  level: INFO
  save_checkpoints: true
metadata:
  description: Default baseline config for gradient search around GAT+RL
  name: gat_rl_gradient_defaults
  version: 1
model:
  architecture: GAT
  attn_dropout: 0.0
  edge_embedding_divisor: 8
  feedforward_multiplier: 4
  hidden_dim: 128
  input_dim: 3
  num_heads: 8
  num_layers: 4
  transformer_dropout: 0.1
optimizer:
  betas:
  - 0.9
  - 0.99
  eps: 1.0e-08
  type: AdamW
problem:
  coord_range: 100
  coordinate_distribution: uniform
  demand_range:
  - 1
  - 10
  num_customers: 20
  type: CVRP
  vehicle_capacity: 30
rl:
  algorithm: PPO
  entropy_coef: 0.005
  gae_lambda: 0.95
  gamma: 0.99
  ppo_clip_range: 0.2
  value_coef: 0.5
training:
  batch_size: 512
  learning_rate: 0.0001
  num_epochs: 128
  num_instances: 4096
  validation_frequency: 8
training_advanced:
  early_stopping_patience: 25
  gradient_accumulation: 1
  lr_schedule_type: cosine
  lr_warmup_steps: auto
  max_grad_norm: 1.0
  weight_decay: 0.0001
